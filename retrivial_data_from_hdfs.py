import os
import json
import numpy as np
import faiss
import spacy
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from typing import List, Dict, Any

# ===== CONFIGURATION =====
load_dotenv()
FAISS_DB_PATH = os.getenv('FAISS_DB_PATH', 'faiss_db/index.index')
GOAL_DATA_DIR = os.getenv('GOAL_DATA_DIR', 'faiss_db')

# Mod√®le spaCy pour la segmentation
try:
    nlp = spacy.load("fr_core_news_sm")
except OSError:
    spacy.cli.download("fr_core_news_sm")
    nlp = spacy.load("fr_core_news_sm")

# ===== FONCTIONS PRINCIPALES =====
def split_chunks(text: str, max_chunk_size: int = 300) -> List[str]:
    """D√©coupe un texte en chunks bas√©s sur des phrases."""
    doc = nlp(text)
    chunks = []
    current_chunk = ""

    for sentence in doc.sents:
        sentence_text = sentence.text.strip()
        if len(current_chunk) + len(sentence_text) <= max_chunk_size:
            current_chunk += " " + sentence_text if current_chunk else sentence_text
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence_text
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def get_embedding_function():
    """Retourne le mod√®le d'embedding multilingue."""
    return SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")

def add_to_faiss(chunks: List[str], embedder: SentenceTransformer, save_path: str = FAISS_DB_PATH) -> Dict[str, Any]:
    """Ajoute les embeddings des chunks √† l'index FAISS."""
    if not chunks:
        return {"status": "error", "message": "Aucun chunk √† indexer."}

    try:
        print("üîπ G√©n√©ration des embeddings...")
        embeddings = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # V√©rification des embeddings
        if np.isnan(embeddings).any():
            raise ValueError("Embeddings contiennent des valeurs NaN")
            
        print("üîπ Normalisation L2...")
        faiss.normalize_L2(embeddings)
        
        print("üîπ Cr√©ation de l'index FAISS...")
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        faiss.write_index(index, save_path)
        
        return {"status": "success", "message": f"{len(chunks)} chunks index√©s."}
        
    except Exception as e:
        return {"status": "error", "message": f"Erreur lors de l'indexation: {str(e)}"}

def save_chunks_to_json(chunks: List[str], goal_id: str) -> bool:
    """Sauvegarde les chunks dans un fichier JSON."""
    try:
        json_path = os.path.join(GOAL_DATA_DIR, f"{goal_id}.json")
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        
        chunks_dict = {str(i): chunk for i, chunk in enumerate(chunks)}
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_dict, f, ensure_ascii=False, indent=2)
            
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {str(e)}")
        return False

def verify_data(goal_id: str = "doc_2_fixed"):
    """V√©rifie et corrige le format des donn√©es"""
    try:
        with open(f"{GOAL_DATA_DIR}/{goal_id}.json", "r") as f:
            data = json.load(f)
        
        # Conversion liste ‚Üí dict si n√©cessaire
        if isinstance(data, list):
            data = {str(i): chunk for i, chunk in enumerate(data)}
            with open(f"{GOAL_DATA_DIR}/{goal_id}.json", "w") as f:
                json.dump(data, f, indent=2)
        
        # V√©rification du contenu
        if not data or len(data) == 0:
            raise ValueError("Le fichier JSON est vide")
            
        first_chunk = list(data.values())[0]
        if not isinstance(first_chunk, str) or len(first_chunk) < 20:
            raise ValueError("Format de chunk invalide")
            
        print(f"‚úÖ Donn√©es v√©rifi√©es ({len(data)} chunks valides)")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur dans les donn√©es: {str(e)}")
        return False

def enhanced_similarity_search(
    query_text: str,
    goal_id: str,
    top_k: int = 5,
    min_score: float = 0.15,  # Seuil tr√®s bas
    max_distance: float = 0.99  # Presque tout accepter
) -> List[Dict[str, Any]]:
    try:
        # Chargement des donn√©es avec v√©rification
        if not verify_data(goal_id):
            return []

        # 1. V√©rification du fichier de donn√©es
        json_path = os.path.join(GOAL_DATA_DIR, f"{goal_id}.json")
        if not os.path.exists(json_path):
            print(f"‚ùå Fichier {json_path} introuvable.")
            return []

        # 2. Chargement des chunks
        with open(json_path, "r", encoding='utf-8') as f:
            chunks_data = json.load(f)
            
        if not chunks_data:
            print("‚ö†Ô∏è Aucun chunk disponible dans le fichier.")
            return []

        # 3. V√©rification de l'index FAISS
        if not os.path.exists(FAISS_DB_PATH):
            print(f"‚ùå Index FAISS introuvable : {FAISS_DB_PATH}")
            return []

        # 4. G√©n√©ration de l'embedding de la requ√™te
        model = get_embedding_function()
        query_embedding = model.encode([query_text], convert_to_numpy=True)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # 5. Recherche dans l'index FAISS
        index = faiss.read_index(FAISS_DB_PATH)
        scores, indices = index.search(query_embedding, top_k * 2)  # Recherche √©largie
        
        # 6. Traitement des r√©sultats
        if len(scores) == 0 or len(indices) == 0:
            print("‚ö†Ô∏è Aucun r√©sultat retourn√© par FAISS.")
            return []

        # Nouvelle strat√©gie de scoring
        scores = np.clip(scores, -1, 1)
        adjusted_scores = (scores + 1) / 2  # Conversion [-1,1] ‚Üí [0,1]
        
        results = []
        for score, idx in zip(adjusted_scores[0], indices[0]):
            chunk_id = str(idx)
            if chunk_id in chunks_data and score >= min_score:
                results.append({
                    "text": chunks_data[chunk_id],
                    "similarity_score": float(score),
                    "metadata": {"source": goal_id, "chunk_id": chunk_id}
                })

        return sorted(results, key=lambda x: x["similarity_score"], reverse=True)[:top_k]

    except Exception as e:
        print(f"‚ùå Erreur critique: {str(e)}")
        return []

def list_available_goals() -> List[str]:
    """Liste les goal_ids disponibles."""
    if not os.path.exists(GOAL_DATA_DIR):
        return []
    return [f.replace('.json', '') for f in os.listdir(GOAL_DATA_DIR) if f.endswith('.json')]

def rebuild_index(goal_id: str = "doc_2_fixed"):
    """Reconstruit compl√®tement l'index FAISS"""
    try:
        # 1. Charger les donn√©es
        with open(f"{GOAL_DATA_DIR}/{goal_id}.json", "r") as f:
            chunks_data = json.load(f)
        
        # 2. V√©rifier le format
        if isinstance(chunks_data, list):
            chunks_data = {str(i): chunk for i, chunk in enumerate(chunks_data)}
        
        chunks = list(chunks_data.values())
        
        # 3. G√©n√©rer les embeddings
        embedder = get_embedding_function()
        result = add_to_faiss(chunks, embedder)
        
        if result["status"] == "success":
            print("‚úÖ Index reconstruit avec succ√®s")
            return True
        else:
            print(f"‚ùå Erreur: {result['message']}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la reconstruction: {str(e)}")
        return False

# ===== TESTS =====
def test_enhanced_search():
    """Teste la recherche am√©lior√©e."""
    print("\n=== TEST DE RECHERCHE ===")
    try:
        # R√©g√©n√©rer l'index si absent
        if not os.path.exists(FAISS_DB_PATH):
            print("‚öôÔ∏è Index FAISS absent. R√©g√©n√©ration en cours...")
            embedder = get_embedding_function()
            with open(f"{GOAL_DATA_DIR}/doc_2_fixed.json", "r") as f:
                chunks_data = json.load(f)
            # Convertir en dictionnaire si c'est une liste
            if isinstance(chunks_data, list):
                chunks_data = {str(i): chunk for i, chunk in enumerate(chunks_data)}
            chunks = list(chunks_data.values())
            add_to_faiss(chunks, embedder)  # Recr√©e l'index
            print("‚úÖ Index FAISS r√©g√©n√©r√©.")
        
        results = enhanced_similarity_search(
            query_text="d√©fis √©thiques IA",
            goal_id="doc_2_fixed",
            min_score=0.1,  # Seuil tr√®s bas pour debug
            max_distance=1.0
        )
        
        if not results:
            print("Aucun r√©sultat trouv√©.")
            return False
        
        print(f"R√©sultats obtenus : {len(results)}")
        for i, res in enumerate(results, 1):
            print(f"\nR√©sultat #{i}:")
            print(f"Score : {res['similarity_score']:.2f}")
            print(f"Contenu : {res['text'][:100]}...")
        return True
    
    except Exception as e:
        print(f"√âchec du test : {str(e)}")
        return False

# ===== POINT D'ENTR√âE =====
if __name__ == "__main__":
    print("=== SYST√àME DE RECHERCHE RAG ===")
    
    # 1. V√©rifier et reconstruire l'index si n√©cessaire
    if not os.path.exists(FAISS_DB_PATH):
        print("‚öôÔ∏è Reconstruction de l'index FAISS...")
        if not rebuild_index():
            exit(1)
    
    # 2. Ex√©cuter la recherche
    results = enhanced_similarity_search(
        query_text="√©thique intelligence artificielle",
        goal_id="doc_2_fixed",
        min_score=0.2,
        max_distance=0.9
    )
    
    if results:
        print("\nüîç R√âSULTATS :")
        for i, res in enumerate(results, 1):
            print(f"\n#{i} [Score: {res['similarity_score']:.2f}]")
            print(res["text"][:200] + ("..." if len(res["text"]) > 200 else ""))
    else:
        print("\n‚ö†Ô∏è Aucun r√©sultat. Suggestions suppl√©mentaires:")
        print("- V√©rifiez que le fichier JSON contient du texte pertinent")
        print("- Essayez d'autres requ√™tes plus simples")
        print("- Inspectez les logs de construction de l'index")

